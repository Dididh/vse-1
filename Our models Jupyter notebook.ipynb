{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "## General libraries\n",
    "from itertools import ifilter\n",
    "import numpy as np\n",
    "import inspect\n",
    "import csv\n",
    "from astropy.table import Table, Column\n",
    "\n",
    "## For machine learning\n",
    "import graphlab\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/tomarnoldussen/Documents/vse/data/train.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/tomarnoldussen/Documents/vse/data/train.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.046733 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.046733 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[int,int,str,int,int,str,str,str,str,str,str,str,str,str,str,str,str,int,int,int,int,str,str,str,str,str,int,str,str,str,str,str,str,str,int,str,int,int,int,str,str,str,str,int,int,int,int,int,int,int,int,int,int,str,int,str,int,str,str,int,str,int,int,str,str,str,int,int,int,int,int,int,str,str,str,int,int,int,str,str,int]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/tomarnoldussen/Documents/vse/data/train.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/tomarnoldussen/Documents/vse/data/train.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 1460 lines in 0.021244 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 1460 lines in 0.021244 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/tomarnoldussen/Documents/vse/data/test.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/tomarnoldussen/Documents/vse/data/test.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.0309 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.0309 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[int,int,str,int,int,str,str,str,str,str,str,str,str,str,str,str,str,int,int,int,int,str,str,str,str,str,int,str,str,str,str,str,str,str,int,str,int,int,int,str,str,str,str,int,int,int,int,int,int,int,int,int,int,str,int,str,int,str,str,int,str,int,int,str,str,str,int,int,int,int,int,int,str,str,str,int,int,int,str,str,int]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/tomarnoldussen/Documents/vse/data/test.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/tomarnoldussen/Documents/vse/data/test.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 1459 lines in 0.022479 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 1459 lines in 0.022479 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load all data sources\n",
    "trainData = graphlab.SFrame('data/train.csv')\n",
    "testData = graphlab.SFrame('data/test.csv')\n",
    "\n",
    "trainData.print_rows(num_rows=10, num_columns=81)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class loadModels():\n",
    "    def tomsModel(self):\n",
    "        #Name of your model\n",
    "        \"\"\"Tom's model\"\"\"\n",
    "        my_features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'zipcode']\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loadModels().tomsModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to evaluate specific models\n",
    "def evaluateModels(models, data):\n",
    "    f = models()\n",
    "    attrs = (getattr(f, name) for name in dir(f))\n",
    "    methods = ifilter(inspect.ismethod, attrs)\n",
    "    tableData = [0] * 100\n",
    "    i = 0\n",
    "    for method in methods:\n",
    "        try:\n",
    "            test = method().evaluate(data)\n",
    "            t = Table([[method.__doc__], [test['max_error']],  [test['rmse']]], names=('Model', 'Max Error', 'RMSE'))\n",
    "            print t\n",
    "            i = i + 1\n",
    "        except TypeError:\n",
    "            # Can't handle methods with required arguments.\n",
    "            print \"error\"\n",
    "            pass\n",
    "#     row_format =\"{:>15}\" * (len(tableColumns) + 1)\n",
    "#     print row_format.format(\"\", *tableColumns)\n",
    "#     for model, row in zip(tableColumns, tableData):\n",
    "#         print row_format.format(\"\", *row)\n",
    "        \n",
    "\n",
    "# Function to create a CSV for testing at Kaggle competition\n",
    "def generateCSVForKaggle(model, data, tag):\n",
    "\n",
    "    with open('output.csv', 'w') as outcsv: \n",
    "        writer = csv.writer(outcsv, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL, lineterminator='\\n')\n",
    "        writer.writerow(['Id', 'SalePrice'])\n",
    "        for i in xrange(0, len(data)):\n",
    "            dataId = data[i]['Id']\n",
    "            dataPrediction = model().tomsModel().predict(data[i])\n",
    "            writer.writerow([dataId, dataPrediction[0]])\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-66644508ce83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Call evaluation function on all models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mevaluateModels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloadModels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-153-45af8eab5fe7>\u001b[0m in \u001b[0;36mevaluateModels\u001b[0;34m(models, data)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmethods\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rmse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Max Error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RMSE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'evaluate'"
     ]
    }
   ],
   "source": [
    "# Call evaluation function on all models\n",
    "\n",
    "evaluateModels(loadModels, trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generateCSVForKaggle(loadModels, testData, 'SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
